{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9f93b4-50a5-4c69-b02a-31260dd44b58",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674a727a-068b-4323-9a22-cbe422ce7144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from absl import logging\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from pathlib import Path\n",
    "import mediapipe as mp\n",
    "import subprocess\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import random\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import fairseq\n",
    "import fairseq.models\n",
    "from fairseq.models import MODEL_REGISTRY\n",
    "from tqdm import tqdm\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "from fairseq.dataclass.configs import GenerationConfig\n",
    "from IPython.display import HTML\n",
    "from argparse import Namespace\n",
    "import torch.nn.functional as F\n",
    "import av_hubert.avhubert\n",
    "import sentencepiece as spm\n",
    "from torch.serialization import safe_globals\n",
    "from fairseq.data.dictionary import Dictionary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import fairseq.criterions.ctc as ctc\n",
    "from jiwer import wer\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13216eaa-5697-44c8-bb81-995c28fe24bb",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed93c3f-2722-44b6-aee8-5f4301a28473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating path variables\n",
    "audio_files_path = Path(\"audio_files\")\n",
    "transcriptions_path = Path(\"transcriptions\")\n",
    "models_path = Path(\"models\")\n",
    "video_files_path = Path(\"video_files\")\n",
    "transcriptions_paths = [file for file in transcriptions_path.iterdir() if file.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbb157-05e0-42cf-b192-a0b749968885",
   "metadata": {},
   "source": [
    "### Training sentencepiece model\n",
    "Training model for splitting words into subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6c6944-aa92-488d-ad38-ee6382ca416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_srt_transcript(transcript_path):\n",
    "        \"\"\"Parses an .srt file and returns a list of words in order.\"\"\"\n",
    "        words = []\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not re.match(r\"^\\d+$\", line) and \"-->\" not in line and line:\n",
    "                words.append(line.lower())\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50291c82-9b36-402b-ac0f-fa166217ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentencepiece(transcripts, vocab_size=1000):\n",
    "        \"\"\"Trains SentencePiece model on the transcripts and returns subword to index dictionary.\"\"\"\n",
    "        temp_file = \"temp_corpus.txt\"\n",
    "        with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for transcript in transcripts:\n",
    "                words = parse_srt_transcript(transcript)\n",
    "                chunk_size = 20  # Number of words per line\n",
    "                chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "                for chunk in chunks:\n",
    "                    f.write(chunk + \"\\n\")\n",
    "        \n",
    "        spm.SentencePieceTrainer.train(input=temp_file, model_prefix=\"sp_model\", vocab_size=vocab_size, model_type=\"bpe\", hard_vocab_limit=False)\n",
    "        sp = spm.SentencePieceProcessor(model_file=\"sp_model.model\")\n",
    "        # Create dictionary of subwords and their corresponding indices\n",
    "        subword_to_idx = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
    "\n",
    "        return sp, subword_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82503ee1-fbba-4b59-9650-8a048423162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: temp_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: sp_model\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: temp_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 1562 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=197671\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9605% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=37\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999605\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 1562 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 1562\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 8793\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4729 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1173 size=20 all=1166 active=1128 piece=ię\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=681 size=40 all=1819 active=1781 piece=go\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=492 size=60 all=2472 active=2434 piece=ni\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=336 size=80 all=3146 active=3108 piece=no\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=266 size=100 all=3995 active=3957 piece=▁prze\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=265 min_freq=20\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=218 size=120 all=4495 active=1469 piece=▁pro\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=172 size=140 all=4840 active=1814 piece=szy\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=144 size=160 all=5312 active=2286 piece=ba\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=127 size=180 all=5739 active=2713 piece=ście\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=113 size=200 all=6054 active=3028 piece=▁właśnie\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=111 min_freq=18\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=101 size=220 all=6381 active=1328 piece=stu\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=92 size=240 all=6689 active=1636 piece=ga\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=85 size=260 all=6991 active=1938 piece=in\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=78 size=280 all=7250 active=2197 piece=▁da\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73 size=300 all=7473 active=2420 piece=bo\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=73 min_freq=15\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=67 size=320 all=7708 active=1205 piece=men\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=61 size=340 all=7909 active=1406 piece=▁teraz\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=56 size=360 all=8099 active=1596 piece=lej\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=53 size=380 all=8358 active=1855 piece=▁fa\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=51 size=400 all=8556 active=2053 piece=▁praw\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=51 min_freq=13\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=48 size=420 all=8724 active=1156 piece=sto\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45 size=440 all=8885 active=1317 piece=sy\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44 size=460 all=9070 active=1502 piece=▁spraw\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=41 size=480 all=9242 active=1674 piece=▁zw\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39 size=500 all=9364 active=1796 piece=▁wo\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39 min_freq=12\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38 size=520 all=9460 active=1086 piece=▁takiego\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=35 size=540 all=9625 active=1251 piece=łą\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33 size=560 all=9748 active=1374 piece=pu\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32 size=580 all=9887 active=1513 piece=▁zmie\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31 size=600 all=9952 active=1578 piece=▁bardziej\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30 min_freq=11\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30 size=620 all=10067 active=1116 piece=▁powiedzieć\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=29 size=640 all=10201 active=1250 piece=▁półno\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28 size=660 all=10314 active=1363 piece=▁powsta\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27 size=680 all=10474 active=1523 piece=▁dewo\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26 size=700 all=10580 active=1629 piece=▁mia\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=26 min_freq=10\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25 size=720 all=10735 active=1150 piece=▁rzą\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24 size=740 all=10884 active=1299 piece=iask\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24 size=760 all=10935 active=1350 piece=▁pytanie\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23 size=780 all=11065 active=1480 piece=▁zoba\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22 size=800 all=11180 active=1595 piece=▁okaz\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=22 min_freq=9\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21 size=820 all=11259 active=1074 piece=imi\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21 size=840 all=11377 active=1192 piece=▁każdy\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20 size=860 all=11502 active=1317 piece=cach\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20 size=880 all=11531 active=1346 piece=▁wszystkich\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19 size=900 all=11659 active=1474 piece=▁map\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=19 min_freq=8\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19 size=920 all=11695 active=1031 piece=▁różnych\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18 size=940 all=11846 active=1182 piece=▁ile\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18 size=960 all=11913 active=1249 piece=▁zrobić\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: sp_model.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: sp_model.vocab\n"
     ]
    }
   ],
   "source": [
    "# Training sentencepiece model for subwords\n",
    "x = train_sentencepiece([transcriptions_path / \"geologia 1.srt\",\n",
    "                    transcriptions_path / \"geologia 2.srt\",\n",
    "                    transcriptions_path / \"geologia_3.srt\",\n",
    "                    transcriptions_path / \"geologia_4.srt\",\n",
    "                    transcriptions_path / \"geologia 5.srt\",\n",
    "                    transcriptions_path / \"geologia 6.srt\",\n",
    "                    transcriptions_path / \"geologia 7.srt\",\n",
    "                    transcriptions_path / \"geologia 8.srt\",\n",
    "                    transcriptions_path / \"geologia 9.srt\",\n",
    "                    transcriptions_path / \"geologia 10.srt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b9c4b-edef-49df-989f-034c693b8d34",
   "metadata": {},
   "source": [
    "# Dataset classes\n",
    "Two dataset classes created for testing purposes. \n",
    "\n",
    "First class DynamicAVHubertDatasetFrames, which outputs every 5th frame from video(to reduce dataset size without loosing meaningfull information) and corresponding subword. This class is used for training with CrossEntropy Loss, which needs precise alignment. \n",
    "\n",
    "Second class DynamicAVHubertDataset which outputs 5 seconds chunks for each created class instance and corresponding transcription chunk. This class is used for training with Ctc Loss, which doesn't need precise alignemnt, however it needs video and transcription sizes. \n",
    "\n",
    "AVHubert was originally trained using Ctc Loss, but for testing purpuses CrossEntropy Loss was also tested. \n",
    "\n",
    "Pipelines for both functions work in a simmilar way. \n",
    "For DynamicAVHubertDatasetFrames class subwords are generated dynamically by splitting transcript words proportionally to their duration based on SentencePiece tokenization. Each frame is matched to its corresponding subword using exact timestamps. To extract features, frames are sampled using ffmpeg at given timestamps, the mouth region is localized using MediaPipe FaceMesh, and the region of interest is resized to a fixed size. Some simple augmentations such as horizontal flipping, brightness and contrast changes can also be applied.\n",
    "\n",
    "For DynamicAVHubertDataset class, for each chunk, frames are extracted at a fixed frame rate, mouth regions are cropped using MediaPipe, and features are stacked into a tensor. Transcriptions are split into subwords using SentencePiece, and the corresponding subword segment for the video chunk is selected based on timing information. Chunk extraction is randomized during training to improve generalization. This class closely follows the original AVHubert training setup, where longer segments and CTC-based training were used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "52eed593-624c-44ac-8e59-6fc48252e84e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicAVHubertDatasetFrames(Dataset):\n",
    "    def __init__(self, video_paths, transcript_paths, sp_model, roi_size=(112, 112), fps=25, frame_interval=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_paths: List of video file paths\n",
    "            transcript_paths: List of transcript file paths\n",
    "            sp_model: SentencePiece model\n",
    "            roi_size: Size of mouth ROI\n",
    "            fps: Original video frames per second\n",
    "            frame_interval: Use every Nth frame (e.g., 5 means 5fps from 25fps)\n",
    "        \"\"\"\n",
    "        self.video_paths = video_paths\n",
    "        self.transcript_paths = transcript_paths\n",
    "        self.roi_size = roi_size\n",
    "        self.sp = sp_model\n",
    "        self.original_fps = fps\n",
    "        self.frame_interval = frame_interval\n",
    "        self.effective_fps = fps / frame_interval\n",
    "        self.frame_duration = 1.0 / self.effective_fps\n",
    "\n",
    "        assert len(video_paths) == len(transcript_paths), \"Video and transcript lists must match\"\n",
    "        assert fps % frame_interval == 0, \"FPS should be divisible by frame interval\"\n",
    "\n",
    "        self.entries = []\n",
    "        self.frame_indices = []  # Stores (entry_idx, frame_idx) tuples\n",
    "        \n",
    "        # Parse each video and transcript\n",
    "        for entry_idx, (video_path, transcript_path) in enumerate(zip(video_paths, transcript_paths)):\n",
    "            word_data = self._parse_srt_with_timings(transcript_path)\n",
    "            duration = self._get_video_duration(video_path)\n",
    "            width = self._get_video_width(video_path)\n",
    "            height = self._get_video_height(video_path)\n",
    "            \n",
    "            subword_data = self._process_word_data_to_subwords(word_data)\n",
    "            \n",
    "            self.entries.append({\n",
    "                'video_path': video_path,\n",
    "                'subword_data': subword_data,\n",
    "                'duration': duration,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "            \n",
    "            # Calculate frame indices for this video\n",
    "            total_frames = int(duration * self.original_fps)\n",
    "            for frame_idx in range(0, total_frames, self.frame_interval):\n",
    "                self.frame_indices.append((entry_idx, frame_idx))\n",
    "\n",
    "        # MediaPipe setup\n",
    "        self.face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "            static_image_mode=False,\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.4\n",
    "        )\n",
    "        self.mouth_landmarks = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "        ]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry_idx, frame_idx = self.frame_indices[idx]\n",
    "        entry_data = self.entries[entry_idx]\n",
    "\n",
    "        # Calculate timestamp for this frame\n",
    "        original_frame_time = frame_idx / self.original_fps\n",
    "        frame_time = idx * self.frame_duration  # More precise timing\n",
    "        \n",
    "        frame = self._extract_single_frame(\n",
    "            entry_data['video_path'],\n",
    "            original_frame_time,\n",
    "            entry_data['width'],\n",
    "            entry_data['height']\n",
    "        )\n",
    "\n",
    "\n",
    "        if frame is None:\n",
    "            return {\n",
    "                'video': torch.zeros((1, *self.roi_size)),\n",
    "                'text': self.sp.pad_id()  # or appropriate blank token\n",
    "            }\n",
    "        \n",
    "        subword = self._find_subword_for_frame(entry_data['subword_data'], frame_time)\n",
    "        token = self.sp.encode(subword, out_type=int)\n",
    "        token = [t for t in token if t != self.sp.piece_to_id('▁')]\n",
    "        \n",
    "        return {\n",
    "            'video': torch.from_numpy(frame).float() / 255.0,\n",
    "            'text': token[0] if len(token) > 0 else self.sp.pad_id()\n",
    "        }\n",
    "\n",
    "\n",
    "    def _augment_mouth_roi(self, roi):\n",
    "        # roi: np.array shape (1, H, W) — grayscale\n",
    "        if random.random() < 0.3:\n",
    "            roi = np.flip(roi, axis=2)  # Horizontal flip\n",
    "        if random.random() < 0.3:\n",
    "            #roi = roi + np.random.normal(0, 5, roi.shape)  # Gaussian noise\n",
    "            roi = np.clip(roi, 0, 255)\n",
    "        if random.random() < 0.3:\n",
    "            alpha = random.uniform(0.8, 1.2)  # Contrast\n",
    "            beta = random.uniform(0, 10)    # Brightness\n",
    "            roi = np.clip(alpha * roi + beta, 0, 255)\n",
    "        return roi\n",
    "\n",
    "    def _find_video_and_frame(self, global_frame_idx):\n",
    "        \"\"\"Find which video and which frame withind that video corresponds to the global frame index\"\"\"\n",
    "        current_frame = 0\n",
    "        for video_idx, entry in enumerate(self.entries):\n",
    "            video_frames = int(entry['duration'] * self.fps)\n",
    "            if global_frame_idx < current_frame + video_frames:\n",
    "                return video_idx, global_frame_idx - current_frame\n",
    "            current_frame += video_frames\n",
    "        return 0, 0  # fallback\n",
    "\n",
    "    def _extract_single_frame(self, video_path, timestamp, width, height):\n",
    "        \"\"\"Extract a single frame at the specified timestamp\"\"\"\n",
    "        cmd = [\n",
    "            'ffmpeg', '-ss', str(timestamp), '-i', str(video_path),\n",
    "            '-vframes', '1',  # Just get one frame\n",
    "            '-f', 'image2pipe', '-pix_fmt', 'rgb24',\n",
    "            '-vcodec', 'rawvideo', '-'\n",
    "        ]\n",
    "        try:\n",
    "            pipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            raw = pipe.stdout.read(width * height * 3)\n",
    "            pipe.terminate()\n",
    "\n",
    "            if not raw:\n",
    "                return None\n",
    "\n",
    "            frame = np.frombuffer(raw, dtype=np.uint8).reshape((height, width, 3))\n",
    "            #plt.imshow(frame[0], cmap='grey')\n",
    "            \n",
    "            mouth_roi = self._extract_mouth_roi(frame, width, height)\n",
    "            return mouth_roi\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frame: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _find_subword_for_frame(self, subword_data, frame_time):\n",
    "        \"\"\"Find the subword that corresponds to the given frame time\"\"\"\n",
    "        for subword in subword_data:\n",
    "            if subword['start'] <= frame_time < subword['end']:\n",
    "                return subword['text']\n",
    "        return \"<space>\"  # Return space token if no subword found\n",
    "\n",
    "    def _process_word_data_to_subwords(self, word_data):\n",
    "        \"\"\"Convert word-level timings to subword-level timings\"\"\"\n",
    "        subword_data = []\n",
    "        \n",
    "        for word in word_data:\n",
    "            word_text = word['text']\n",
    "            start_time = word['start']\n",
    "            end_time = word['end']\n",
    "            word_duration = end_time - start_time\n",
    "            \n",
    "            # Encode the word into subwords\n",
    "            subwords = self.sp.encode_as_pieces(word_text)\n",
    "            num_subwords = len(subwords)\n",
    "            \n",
    "            if num_subwords == 0:\n",
    "                continue  # Skip empty words\n",
    "                \n",
    "            # Calculate the total length of all subwords in characters\n",
    "            total_subword_length = sum(len(subword.replace(\"▁\", \"\")) for subword in subwords)\n",
    "            \n",
    "            # Distribute the word's duration proportionally to the length of each subword\n",
    "            original_start_time = start_time\n",
    "            for subword in subwords:\n",
    "                subword_duration = word_duration * (len(subword.replace(\"▁\", \"\")) / total_subword_length)\n",
    "                subword_end = start_time + subword_duration\n",
    "                \n",
    "                subword_data.append({\n",
    "                    'text': subword,\n",
    "                    'start': start_time,\n",
    "                    'end': subword_end\n",
    "                })\n",
    "                # Update the start time for the next subword\n",
    "                start_time = subword_end\n",
    "                \n",
    "        return subword_data\n",
    "\n",
    "    def _parse_srt_with_timings(self, srt_path):\n",
    "        \"\"\"Parse the SRT file into word-level timings\"\"\"\n",
    "        with open(srt_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "        word_data = []\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if re.match(r\"\\d+:\\d+:\\d+,\\d+ --> \\d+:\\d+:\\d+,\\d+\", lines[i]):\n",
    "                start, end = lines[i].split(' --> ')\n",
    "                start_sec = self._srt_time_to_seconds(start.strip())\n",
    "                end_sec = self._srt_time_to_seconds(end.strip())\n",
    "                \n",
    "                # Get all text lines until next timestamp or empty line\n",
    "                i += 1\n",
    "                text_lines = []\n",
    "                while i < len(lines) and not re.match(r\"(\\d+:\\d+:\\d+,\\d+ -->|^\\d+$)\", lines[i]):\n",
    "                    text_lines.append(lines[i])\n",
    "                    i += 1\n",
    "                \n",
    "                # Split into words with original timings\n",
    "                if text_lines:\n",
    "                    full_text = ' '.join(text_lines)\n",
    "                    words = full_text.split()\n",
    "                    word_duration = (end_sec - start_sec) / len(words)\n",
    "                    \n",
    "                    for j, word in enumerate(words):\n",
    "                        word_data.append({\n",
    "                            'text': word.lower(),\n",
    "                            'start': start_sec + j * word_duration,\n",
    "                            'end': start_sec + (j + 1) * word_duration\n",
    "                        })\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return word_data\n",
    "\n",
    "    def _srt_time_to_seconds(self, time_str):\n",
    "        \"\"\"Convert SRT timestamp to seconds\"\"\"\n",
    "        hh_mm_ss, ms = time_str.split(',')  # hh:mm:ss,xxx\n",
    "        h, m, s = hh_mm_ss.split(':')\n",
    "        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "\n",
    "    def _get_video_duration(self, video_path):\n",
    "        \"\"\"Get video duration in seconds using ffprobe\"\"\"\n",
    "        cmd = [\n",
    "            'ffprobe', '-i', str(video_path),\n",
    "            '-show_entries', 'format=duration',\n",
    "            '-v', 'quiet', '-of', 'csv=p=0'\n",
    "        ]\n",
    "        try:\n",
    "            duration = float(subprocess.check_output(cmd).decode().strip())\n",
    "            return duration\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting duration: {e}\")\n",
    "            return 0.0  # Fallback value\n",
    "\n",
    "    def _get_video_width(self, video_path):\n",
    "        \"\"\"Get video width using ffprobe\"\"\"\n",
    "        cmd = [\n",
    "            'ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "            '-show_entries', 'stream=width', '-of', 'csv=p=0', str(video_path)\n",
    "        ]\n",
    "        return int(subprocess.check_output(cmd).decode().strip())\n",
    "\n",
    "    def _get_video_height(self, video_path):\n",
    "        \"\"\"Get video height using ffprobe\"\"\"\n",
    "        cmd = [\n",
    "            'ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "            '-show_entries', 'stream=height', '-of', 'csv=p=0', str(video_path)\n",
    "        ]\n",
    "        return int(subprocess.check_output(cmd).decode().strip())\n",
    "\n",
    "    def _extract_mouth_roi(self, frame, width, height):\n",
    "        \"\"\"Extract the mouth ROI from a frame\"\"\"\n",
    "        results = self.face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        #plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            return None\n",
    "    \n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        mouth_points = np.array([(landmarks[i].x, landmarks[i].y) \n",
    "                                for i in self.mouth_landmarks])\n",
    "    \n",
    "        # Dynamic bounding box with padding\n",
    "        x_min, y_min = mouth_points.min(axis=0)\n",
    "        x_max, y_max = mouth_points.max(axis=0)\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        padding = max(w, h) * 0.2  # 20% padding\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        x_min = int(max(0, (x_min - padding) * width))\n",
    "        y_min = int(max(0, (y_min - padding) * height))\n",
    "        x_max = int(min(width, (x_max + padding) * width))\n",
    "        y_max = int(min(height, (y_max + padding) * height))\n",
    "        \n",
    "        # Extract and resize\n",
    "        mouth_roi = frame[y_min:y_max, x_min:x_max]\n",
    "        if mouth_roi.size == 0:\n",
    "            return None\n",
    "\n",
    "        mouth_roi = cv2.cvtColor(mouth_roi, cv2.COLOR_BGR2GRAY)  # -> (H, W)\n",
    "        mouth_roi = cv2.resize(mouth_roi, self.roi_size)         # -> (H, W)\n",
    "        mouth_roi = np.expand_dims(mouth_roi, axis=0) \n",
    "        return mouth_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9632a9a3-4b33-4d49-9b24-a61ea7273099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745663551.028430  860211 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745663551.079197 2063231 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video': tensor([[[0.3647, 0.3725, 0.4000,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.3647, 0.3725, 0.4000,  ..., 0.4078, 0.4078, 0.4078],\n",
      "         [0.3686, 0.3765, 0.4000,  ..., 0.4118, 0.4078, 0.4078],\n",
      "         ...,\n",
      "         [0.4706, 0.4667, 0.4588,  ..., 0.5294, 0.5294, 0.5294],\n",
      "         [0.4706, 0.4667, 0.4588,  ..., 0.5294, 0.5294, 0.5294],\n",
      "         [0.4706, 0.4667, 0.4588,  ..., 0.5294, 0.5294, 0.5294]]]), 'text': 255}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ba1fa146620>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_frames_dataset = train_dataset = DynamicAVHubertDatasetFrames(\n",
    "    video_paths=[video_files_path / \"train_1min.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"train 1min.srt\"],\n",
    "    sp_model=sp\n",
    ")\n",
    "test_sample = test_frames_dataset[100]\n",
    "print(test_sample)\n",
    "plt.imshow(test_sample[\"video\"][0], cmap='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "5aee4596-7851-4cce-9a1c-4ebd753d3ee7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DynamicAVHubertDataset(Dataset):\n",
    "    def __init__(self, video_paths, transcript_paths, sp_model, chunk_size=5.0, roi_size=(112, 112), deterministic=False):\n",
    "        self.video_paths = video_paths\n",
    "        self.transcript_paths = transcript_paths\n",
    "        self.chunk_size = chunk_size\n",
    "        self.roi_size = roi_size\n",
    "        self.sp = sp_model\n",
    "        self.deterministic = deterministic\n",
    "        self.chunk_indices = []\n",
    "        \n",
    "        # Check if the lengths of both lists match\n",
    "        assert len(video_paths) == len(transcript_paths), \"Video and transcript lists must have the same length.\"\n",
    "\n",
    "        self.entries = []\n",
    "        \n",
    "        # Parse each video and transcript\n",
    "        for entry_idx, (video_path, transcript_path) in enumerate(zip(video_paths, transcript_paths)):\n",
    "            word_data = self._parse_srt_with_timings(transcript_path)\n",
    "            duration = self._get_video_duration(video_path)\n",
    "            width = self._get_video_width(video_path)\n",
    "            height = self._get_video_height(video_path)\n",
    "            \n",
    "            self.entries.append({\n",
    "                'video_path': video_path,\n",
    "                'transcript_path': transcript_path,\n",
    "                'word_data': word_data,\n",
    "                'duration': duration,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "\n",
    "            num_chunks = int(duration//self.chunk_size)\n",
    "            if deterministic:\n",
    "                for i in range(num_chunks):\n",
    "                    self.chunk_indices.append((entry_idx, i * self.chunk_size))\n",
    "        # MediaPipe setup\n",
    "        self.face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "            static_image_mode=False,\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "        self.mouth_landmarks = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,  # Outer lips\n",
    "            291, 375, 321, 405, 314, 17, 84, 181, 91, 146  # Inner mouth\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of valid chunks across all videos\"\"\"\n",
    "        if self.deterministic:\n",
    "            return len(self.chunk_indices)\n",
    "        return sum(int(entry['duration'] // self.chunk_size) for entry in self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a random sample from one of the video-transcript pairs\"\"\"\n",
    "        if self.deterministic:\n",
    "            entry_idx, start_time = self.chunk_indices[idx]\n",
    "            entry_data = self.entries[entry_idx]\n",
    "        else:\n",
    "            entry_data = random.choice(self.entries)\n",
    "            duration = entry_data['duration']\n",
    "            start_time = random.uniform(0, duration - self.chunk_size)\n",
    "\n",
    "        end_time = start_time + self.chunk_size\n",
    "        # Extract frames for the chunk\n",
    "        frames = self._extract_frames(entry_data['video_path'], start_time, end_time, entry_data['width'], entry_data['height'])\n",
    "        \n",
    "        if len(frames) < 10:  # Minimum 10 frames\n",
    "            if self.deterministic:\n",
    "                return self.__getitem__(idx+1)\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))  # Retry with a new index\n",
    "\n",
    "        # Find the corresponding words in the transcript for this chunk\n",
    "        word_data = entry_data['word_data']\n",
    "        chunk_words = []\n",
    "        for word in word_data:\n",
    "            if word['start'] >= start_time and word['end'] <= end_time:\n",
    "                chunk_words.append(word['text'])\n",
    "\n",
    "        # Remove punctuation from the words\n",
    "        clean_text = \" \".join(chunk_words)\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', clean_text)  # Remove punctuation\n",
    "        \n",
    "        # Tokenize the word and prepare the text for the model\n",
    "        tokens = self.sp.encode(clean_text, out_type=str)\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        total_video_duration = sum(entry['duration'] for entry in self.entries)\n",
    "        total_chunks = total_video_duration / self.chunk_size\n",
    "\n",
    "        return {\n",
    "            'video': torch.stack([torch.from_numpy(f).float() / 255.0 for f in frames]),  # (T, 3, H, W)\n",
    "            'text': text,\n",
    "            'length': len(frames)\n",
    "        }\n",
    "\n",
    "    def _get_video_duration(self, video_path):\n",
    "        \"\"\"Get video duration in seconds using ffprobe\"\"\"\n",
    "        cmd = [\n",
    "            'ffprobe', '-i', str(video_path),\n",
    "            '-show_entries', 'format=duration',\n",
    "            '-v', 'quiet', '-of', 'csv=p=0'\n",
    "        ]\n",
    "        try:\n",
    "            duration = float(subprocess.check_output(cmd).decode().strip())\n",
    "            return duration\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting duration: {e}\")\n",
    "            return 0.0  # Fallback value\n",
    "\n",
    "    def _get_video_width(self, video_path):\n",
    "        \"\"\"Get video width using ffprobe\"\"\"\n",
    "        cmd = [\n",
    "            'ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "            '-show_entries', 'stream=width', '-of', 'csv=p=0', str(video_path)\n",
    "        ]\n",
    "        return int(subprocess.check_output(cmd).decode().strip())\n",
    "\n",
    "    def _get_video_height(self, video_path):\n",
    "        \"\"\"Get video height using ffprobe\"\"\"\n",
    "        cmd = [\n",
    "            'ffprobe', '-v', 'error', '-select_streams', 'v:0',\n",
    "            '-show_entries', 'stream=height', '-of', 'csv=p=0', str(video_path)\n",
    "        ]\n",
    "        return int(subprocess.check_output(cmd).decode().strip())\n",
    "\n",
    "    def _extract_frames(self, video_path, start_time, end_time, width, height):\n",
    "        \"\"\"Extract frames for the given time range\"\"\"\n",
    "        cmd = [\n",
    "            'ffmpeg', '-ss', str(start_time), '-i', str(video_path),\n",
    "            '-t', str(end_time - start_time), '-r', '25',  # 25 FPS\n",
    "            '-f', 'image2pipe', '-pix_fmt', 'rgb24',\n",
    "            '-vcodec', 'rawvideo', '-'\n",
    "        ]\n",
    "        pipe = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        frames = []\n",
    "    \n",
    "        while True:\n",
    "            raw = pipe.stdout.read(width * height * 3)\n",
    "            if not raw:\n",
    "                break\n",
    "            frame = np.frombuffer(raw, dtype=np.uint8).reshape((height, width, 3))\n",
    "            \n",
    "            # Now pass width and height to _extract_mouth_roi\n",
    "            mouth_roi = self._extract_mouth_roi(frame, width, height)\n",
    "            if mouth_roi is not None:\n",
    "                frames.append(mouth_roi)\n",
    "        \n",
    "        pipe.terminate()\n",
    "        return frames\n",
    "\n",
    "    def _extract_mouth_roi(self, frame, width, height):\n",
    "        \"\"\"Extract the mouth ROI from a frame\"\"\"\n",
    "        results = self.face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            return None\n",
    "    \n",
    "        landmarks = results.multi_face_landmarks[0].landmark\n",
    "        mouth_points = np.array([(landmarks[i].x, landmarks[i].y) \n",
    "                                for i in self.mouth_landmarks])\n",
    "    \n",
    "        # Dynamic bounding box with padding\n",
    "        x_min, y_min = mouth_points.min(axis=0)\n",
    "        x_max, y_max = mouth_points.max(axis=0)\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        padding = max(w, h) * 0.2  # 20% padding\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        x_min = int(max(0, (x_min - padding) * width))\n",
    "        y_min = int(max(0, (y_min - padding) * height))\n",
    "        x_max = int(min(width, (x_max + padding) * width))\n",
    "        y_max = int(min(height, (y_max + padding) * height))\n",
    "        \n",
    "        # Extract and resize\n",
    "        mouth_roi = frame[y_min:y_max, x_min:x_max]\n",
    "        if mouth_roi.size == 0:\n",
    "            return None\n",
    "\n",
    "        mouth_roi = cv2.cvtColor(mouth_roi, cv2.COLOR_BGR2GRAY)  # -> (H, W)\n",
    "        mouth_roi = cv2.resize(mouth_roi, self.roi_size)         # -> (H, W)\n",
    "        mouth_roi = np.expand_dims(mouth_roi, axis=0) \n",
    "        return mouth_roi\n",
    "\n",
    "    def _parse_srt_with_timings(self, srt_path):\n",
    "        \"\"\"Parse the SRT file into word-level timings\"\"\"\n",
    "        with open(srt_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "        word_data = []\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if re.match(r\"\\d+:\\d+:\\d+,\\d+ --> \\d+:\\d+:\\d+,\\d+\", lines[i]):\n",
    "                start, end = lines[i].split(' --> ')\n",
    "                start_sec = self._srt_time_to_seconds(start.strip())\n",
    "                end_sec = self._srt_time_to_seconds(end.strip())\n",
    "                \n",
    "                # Get all text lines until next timestamp or empty line\n",
    "                i += 1\n",
    "                text_lines = []\n",
    "                while i < len(lines) and not re.match(r\"(\\d+:\\d+:\\d+,\\d+ -->|^\\d+$)\", lines[i]):\n",
    "                    text_lines.append(lines[i])\n",
    "                    i += 1\n",
    "                \n",
    "                # Split into words with original timings\n",
    "                if text_lines:\n",
    "                    full_text = ' '.join(text_lines)\n",
    "                    words = full_text.split()\n",
    "                    word_duration = (end_sec - start_sec) / len(words)\n",
    "                    \n",
    "                    for j, word in enumerate(words):\n",
    "                        word_data.append({\n",
    "                            'text': word.lower(),\n",
    "                            'start': start_sec + j * word_duration,\n",
    "                            'end': start_sec + (j + 1) * word_duration\n",
    "                        })\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return word_data\n",
    "\n",
    "    def _srt_time_to_seconds(self, time_str):\n",
    "        \"\"\"Convert SRT timestamp to seconds\"\"\"\n",
    "        hh_mm_ss, ms = time_str.split(',')  # hh:mm:ss,xxx\n",
    "        h, m, s = hh_mm_ss.split(':')\n",
    "        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "49d1ff23-7658-4203-bfe2-559d654df0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745663716.485732  860211 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745663716.536691 2065130 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video': tensor([[[[0.4314, 0.4314, 0.4314,  ..., 0.4431, 0.4392, 0.4392],\n",
      "          [0.4314, 0.4314, 0.4314,  ..., 0.4392, 0.4392, 0.4392],\n",
      "          [0.4314, 0.4314, 0.4314,  ..., 0.4392, 0.4392, 0.4392],\n",
      "          ...,\n",
      "          [0.3961, 0.3961, 0.3961,  ..., 0.4784, 0.4745, 0.4745],\n",
      "          [0.3961, 0.3961, 0.3961,  ..., 0.4784, 0.4745, 0.4745],\n",
      "          [0.3961, 0.3961, 0.3961,  ..., 0.4784, 0.4745, 0.4745]]],\n",
      "\n",
      "\n",
      "        [[[0.3373, 0.3451, 0.3608,  ..., 0.4000, 0.3961, 0.3961],\n",
      "          [0.3373, 0.3451, 0.3608,  ..., 0.4000, 0.3961, 0.3961],\n",
      "          [0.3373, 0.3451, 0.3608,  ..., 0.4039, 0.4000, 0.4000],\n",
      "          ...,\n",
      "          [0.4706, 0.4667, 0.4627,  ..., 0.4941, 0.4824, 0.4784],\n",
      "          [0.4706, 0.4667, 0.4627,  ..., 0.4941, 0.4824, 0.4784],\n",
      "          [0.4706, 0.4667, 0.4627,  ..., 0.4941, 0.4824, 0.4784]]],\n",
      "\n",
      "\n",
      "        [[[0.4863, 0.4745, 0.4549,  ..., 0.4471, 0.4667, 0.4745],\n",
      "          [0.4863, 0.4745, 0.4549,  ..., 0.4471, 0.4667, 0.4745],\n",
      "          [0.4824, 0.4706, 0.4510,  ..., 0.4471, 0.4627, 0.4745],\n",
      "          ...,\n",
      "          [0.4980, 0.4980, 0.4902,  ..., 0.5294, 0.5216, 0.5176],\n",
      "          [0.4980, 0.4941, 0.4902,  ..., 0.5255, 0.5216, 0.5176],\n",
      "          [0.4980, 0.4941, 0.4902,  ..., 0.5255, 0.5216, 0.5176]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4549, 0.4471, 0.4275,  ..., 0.4627, 0.4667, 0.4706],\n",
      "          [0.4549, 0.4431, 0.4275,  ..., 0.4627, 0.4667, 0.4706],\n",
      "          [0.4510, 0.4392, 0.4235,  ..., 0.4627, 0.4667, 0.4706],\n",
      "          ...,\n",
      "          [0.5647, 0.5569, 0.5412,  ..., 0.5412, 0.5333, 0.5333],\n",
      "          [0.5647, 0.5569, 0.5412,  ..., 0.5373, 0.5373, 0.5333],\n",
      "          [0.5647, 0.5569, 0.5412,  ..., 0.5373, 0.5333, 0.5333]]],\n",
      "\n",
      "\n",
      "        [[[0.4706, 0.4588, 0.4431,  ..., 0.4784, 0.4863, 0.4941],\n",
      "          [0.4706, 0.4588, 0.4431,  ..., 0.4784, 0.4863, 0.4941],\n",
      "          [0.4667, 0.4549, 0.4353,  ..., 0.4745, 0.4863, 0.4902],\n",
      "          ...,\n",
      "          [0.5686, 0.5608, 0.5490,  ..., 0.5333, 0.5294, 0.5294],\n",
      "          [0.5686, 0.5608, 0.5490,  ..., 0.5333, 0.5294, 0.5294],\n",
      "          [0.5686, 0.5608, 0.5490,  ..., 0.5333, 0.5294, 0.5294]]],\n",
      "\n",
      "\n",
      "        [[[0.4745, 0.4667, 0.4510,  ..., 0.4627, 0.4667, 0.4706],\n",
      "          [0.4745, 0.4667, 0.4510,  ..., 0.4627, 0.4667, 0.4706],\n",
      "          [0.4667, 0.4588, 0.4431,  ..., 0.4627, 0.4667, 0.4667],\n",
      "          ...,\n",
      "          [0.5647, 0.5608, 0.5490,  ..., 0.5451, 0.5412, 0.5412],\n",
      "          [0.5647, 0.5608, 0.5490,  ..., 0.5451, 0.5451, 0.5451],\n",
      "          [0.5647, 0.5608, 0.5490,  ..., 0.5451, 0.5451, 0.5451]]]]), 'text': '▁to ▁jest ▁we d łu g ▁mnie', 'length': 22}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ba178004cd0>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = DynamicAVHubertDataset(\n",
    "    video_paths=[video_files_path / \"train_1min.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"train 1min.srt\"],\n",
    "    sp_model=sp,\n",
    "    deterministic=True,\n",
    "    chunk_size=5\n",
    ")\n",
    "test_sample = test_dataset[0]\n",
    "print(test_sample)\n",
    "plt.imshow(test_sample[\"video\"][0][0], cmap='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a631594-5161-4197-a921-aeaf49e9fe79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Models\n",
    "Two Model classes where created. Polish AVHubert for working with Ctc Loss, specificly allowing for multiple outputs. And PolishAVHubertSingleOutput for working with CrossEntropy Loss, where only one output per frame is allowed.\n",
    "\n",
    "The PolishAVHubert class extends the AVHubert architecture by incorporating a SentencePiece model to process Polish subword tokens, making it capable of handling speech and visual data in the Polish language. The model starts with a pre-trained AVHubert, with its parameters frozen at initialization to preserve learned audio-visual features. The feature extractor for video is enabled for gradient updates.\n",
    "\n",
    "During initialization, the model loads a SentencePiece processor from a specified path, which is used to determine the Polish vocabulary size for the final output layer. The model adds a new linear projection layer to map the encoder’s output to this Polish vocabulary size. Furthermore, to allow the model to adapt, the last 4 transformer layers of the encoder are unfrozen, enabling the network to fine-tune its higher-level representations while keeping lower layers fixed. The output projection layer includes a dropout for regularization.\n",
    "\n",
    "In the forward method, the video input is passed through the feature extractor of the AVHubert encoder, and its output is permuted to match the expected format for the transformer layers. The result is then projected to the Polish token space through the output layer, and logits are returned. A dummy padding mask is also generated since padding is not explicitly handled. This model is designed for sequence-to-sequence tasks, outputting token predictions across time steps.\n",
    "\n",
    "\n",
    "The PolishAVHubertSingleOutput class builds upon the same core structure but differs in its approach to output generation. Instead of generating token predictions across time steps, it aggregates the features over time using mean pooling to output a single prediction per frame. Like the original, the model begins by freezing the base AVHubert model and loading the SentencePiece processor. A new output projection layer is added to map the encoder’s output to the Polish subword space, similar to the previous model.\n",
    "\n",
    "In the forward method, video features are extracted in the same way, but after passing through the encoder, the output is pooled across the time dimension (using mean pooling), which reduces the output to a single token prediction per batch item. This design is particularly useful for tasks where only a single token per frame is required, such as single-frame classification tasks. The logits are then projected to the Polish vocabulary space, and the model returns the predictions along with a fixed output length of 1 for each batch item.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c77e4052-69a4-45c4-af5b-2a3f9eebc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolishAVHubert(nn.Module):\n",
    "    def __init__(self, base_model, sp_model_path):\n",
    "        super().__init__()\n",
    "        self.avhubert = base_model\n",
    "        self.avhubert.requires_grad_(False)  # Freeze initially\n",
    "        \n",
    "        # Initialize SentencePiece processor\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(sp_model_path)\n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        \n",
    "        # Get embedding dimension from encoder\n",
    "        embed_dim = self.avhubert.encoder.w2v_model.encoder.layers[0].self_attn.k_proj.in_features\n",
    "        \n",
    "        # Replace output layer for Polish tokens\n",
    "        self.output_proj = nn.Linear(embed_dim, self.sp.vocab_size())  # +1 for blank\n",
    "        \n",
    "        # Unfreeze last 4 transformer layers\n",
    "        for layer in self.avhubert.encoder.w2v_model.encoder.layers[-4:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def get_normalized_probs(self, net_output, log_probs=False):\n",
    "        \"\"\"Handle both dict and tensor inputs for compatibility\"\"\"\n",
    "        if isinstance(net_output, dict):\n",
    "            logits = net_output[\"encoder_out\"][0]  # (T, B, C)\n",
    "        else:\n",
    "            logits = net_output\n",
    "            \n",
    "        if log_probs:\n",
    "            return F.log_softmax(logits, dim=-1)\n",
    "        else:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def forward(self, video):\n",
    "        # Extract video features\n",
    "        self.avhubert.encoder.w2v_model.feature_extractor_video.requires_grad_(True)\n",
    "\n",
    "        features = self.avhubert.encoder.w2v_model.feature_extractor_video(video)\n",
    "        \n",
    "        # Get encoder output\n",
    "        features_permuted = features.permute(0, 2, 1)  # (B, C, T) -> (B, T, C)\n",
    "        encoder_out = self.avhubert.encoder.w2v_model.encoder(features_permuted)[0]\n",
    "        # Project to Polish token space\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        logits = self.output_proj(encoder_out)  # (B, T, vocab_size)\n",
    "        logits = logits.permute(1, 0, 2)  # (T, B, vocab_size)\n",
    "        \n",
    "        # Create padding mask (assuming no padding for now)\n",
    "        B, T, _ = encoder_out.shape\n",
    "        padding_mask = torch.zeros(B, T, dtype=torch.bool, device=logits.device)\n",
    "        \n",
    "        return {\n",
    "            \"encoder_out\": [logits],  # Wrapped in list for compatibility\n",
    "            \"padding_mask\": padding_mask,\n",
    "            \"output_lengths\": torch.full((B,), T, dtype=torch.long, device=logits.device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6a2c3801-2d67-41ba-92c2-3074a3a88eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolishAVHubertSingleOutput(nn.Module):\n",
    "    def __init__(self, base_model, sp_model_path):\n",
    "        super().__init__()\n",
    "        self.avhubert = base_model\n",
    "        self.avhubert.requires_grad_(False)  # Freeze initially\n",
    "        # Initialize SentencePiece processor\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        #self.pad_id = self.sp.pad_id() \n",
    "        self.sp.load(sp_model_path)\n",
    "\n",
    "        # Get embedding dimension from encoder\n",
    "        embed_dim = self.avhubert.encoder.w2v_model.encoder.layers[0].self_attn.k_proj.in_features\n",
    "\n",
    "        # Replace output layer for Polish tokens - now outputs single prediction per frame\n",
    "        self.output_proj = nn.Linear(embed_dim, self.sp.vocab_size())  # +1 for blank\n",
    "\n",
    "        # Unfreeze last 4 transformer layers\n",
    "        for layer in self.avhubert.encoder.w2v_model.encoder.layers[-4:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def get_normalized_probs(self, net_output, log_probs=False):\n",
    "        \"\"\"Get probabilities for the single output prediction\"\"\"\n",
    "        if isinstance(net_output, dict):\n",
    "            logits = net_output[\"encoder_out\"]  # (B, 1, vocab_size)\n",
    "        else:\n",
    "            logits = net_output\n",
    "            \n",
    "        if log_probs:\n",
    "            return F.log_softmax(logits, dim=-1)\n",
    "        else:\n",
    "            return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def forward(self, video):\n",
    "        # Extract video features\n",
    "        self.avhubert.encoder.w2v_model.feature_extractor_video.requires_grad_(True)\n",
    "        features = self.avhubert.encoder.w2v_model.feature_extractor_video(video)\n",
    "        \n",
    "        # Get encoder output - now using mean pooling across time\n",
    "        features_permuted = features.permute(0, 2, 1)  # (B, C, T) -> (B, T, C)\n",
    "        encoder_out = self.avhubert.encoder.w2v_model.encoder(features_permuted)[0]\n",
    "        \n",
    "        # Mean pooling across time dimension to get single representation\n",
    "        pooled = encoder_out.mean(dim=1, keepdim=True)  # (B, 1, C)\n",
    "        \n",
    "        # Project to Polish token space - single prediction\n",
    "        logits = self.output_proj(pooled)  # (B, 1, vocab_size)\n",
    "        \n",
    "        return {\n",
    "            \"encoder_out\": logits,  # Single prediction per batch item\n",
    "            \"padding_mask\": None,\n",
    "            \"output_lengths\": torch.ones(video.size(0))  # Length 1 for each item\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a09c6c-f595-4a85-b435-6c71fa60ca74",
   "metadata": {},
   "source": [
    "# Training preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b0ed3649-f426-41bc-bf94-20be3577ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating paths to loaded models\n",
    "DATA_DIR = \"data\"\n",
    "USER_DIR = \"av_hubert/avhubert\"\n",
    "CHECKPOINT_PATH = os.path.join(DATA_DIR, \"finetune-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "c24b2ffa-37fe-4aed-90c4-9794fcb89775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for training parameters\n",
    "class Config:\n",
    "    # Training params\n",
    "    batch_size = 8\n",
    "    num_epochs = 80\n",
    "    learning_rate = 3e-4\n",
    "    max_grad_norm = 1.0\n",
    "    min_frames = 10\n",
    "    early_stop_patience = 5\n",
    "    resume_training=False\n",
    "    # Paths\n",
    "    checkpoint_path = \"avhubert_pl_checkpoint.pt\"\n",
    "    spm_model_path = \"sp_model.model\"  # Your SentencePiece model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "711765ba-0455-40dd-81d4-200961d123fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom implementation of Ctc Loss\n",
    "from dataclasses import dataclass\n",
    "from fairseq.criterions.ctc import CtcCriterionConfig, CtcCriterion\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "@dataclass\n",
    "class CustomCtcConfig(CtcCriterionConfig):\n",
    "    post_process: str = None # Add missing required field\n",
    "    zero_infinity: bool = True  # Enable zero infinity\n",
    "    sentence_avg: bool = True  # Use sentence averaging\n",
    "    reduction: str = \"mean\"\n",
    "    blank_idx: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a41b4cb8-1460-4dbd-86d9-3fd9fc42733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(texts, sp_model):\n",
    "    \"\"\"Changing list of text to SentencePiece IDs and lengths\"\"\"\n",
    "    encoded = []\n",
    "    for text in texts:\n",
    "        pieces = sp_model.encode(text, out_type=str)\n",
    "        ids = [sp_model.piece_to_id(p) for p in pieces] \n",
    "        encoded.append(torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "    lengths = torch.tensor([len(e) for e in encoded], dtype=torch.long)\n",
    "    padded = pad_sequence(encoded, batch_first=True, padding_value=-1) \n",
    "    return padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9cd0ed4b-3ef8-4116-902a-aa774b0d5926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Filter out empty text samples\n",
    "    batch = [b for b in batch if b['text'].strip()]\n",
    "    \n",
    "    if not batch:  # All samples were empty\n",
    "        return {\n",
    "            'video': torch.zeros((1, 10, 1, 112, 112)),\n",
    "            'text': [\"[UNK]\"],\n",
    "            'lengths': torch.tensor([10])\n",
    "        }\n",
    "    \n",
    "    max_len = max(b['video'].shape[0] for b in batch)\n",
    "    video_padded = []\n",
    "    lengths = []\n",
    "    \n",
    "    for b in batch:\n",
    "        v = b['video']\n",
    "        pad_len = max_len - v.shape[0]\n",
    "        padded_v = torch.cat([v, torch.full((pad_len, 1, 112, 112), 0)], dim=0)\n",
    "        video_padded.append(padded_v)\n",
    "        lengths.append(v.shape[0])\n",
    "    \n",
    "    return {\n",
    "        'video': torch.stack(video_padded),\n",
    "        'text': [b['text'] for b in batch],\n",
    "        'lengths': torch.tensor(lengths)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f2ca031c-24a8-4336-8608-bf7b74209669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_frames(batch):\n",
    "    # Filter out invalid samples\n",
    "    batch = [b for b in batch if b['video'] is not None and b['text'] is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    # Stack videos and get texts\n",
    "    videos = torch.stack([sample['video'] for sample in batch])  # (B, 1, H, W)\n",
    "    texts = [sample['text'] for sample in batch]\n",
    "    \n",
    "    return {\n",
    "        'videos': videos,\n",
    "        'texts': texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "bc365d3f-46b7-47fe-98cd-bee0c0daabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(log_probs, sp_model, input_lengths=None, blank_id=0):\n",
    "    \"\"\"\n",
    "    Convert model outputs to text predictions using CTC decoding rules:\n",
    "    - remove blank tokens (blank_id),\n",
    "    - collapse repeated tokens,\n",
    "    - decode remaining subword IDs into text using sp_model.\n",
    "    \"\"\"\n",
    "    log_probs = log_probs.permute(1, 0, 2)  # (T, B, C) -> (B, T, C)\n",
    "    predictions = []\n",
    "\n",
    "    for i, probs in enumerate(log_probs):\n",
    "        if input_lengths is not None:\n",
    "            probs = probs[:input_lengths[i]]\n",
    "\n",
    "        predicted_ids = torch.argmax(probs, dim=-1)\n",
    "\n",
    "        decoded_ids = []\n",
    "        prev_id = None\n",
    "        for id in predicted_ids:\n",
    "            id = id.item()\n",
    "            if id != blank_id and id != prev_id:\n",
    "                decoded_ids.append(id - 1)  # shift back: model output was shifted +1\n",
    "            prev_id = id\n",
    "        for id in decoded_ids:\n",
    "            if not (0 <= id < sp_model.vocab_size()):\n",
    "                print(f\"Invalid token ID after shift: {id}\")\n",
    "        # Now it's safe to decode the original SP IDs\n",
    "        tokens = [sp_model.id_to_piece(id) for id in decoded_ids if 0 <= id < sp_model.vocab_size()]\n",
    "        text = sp_model.decode_pieces(tokens)\n",
    "        predictions.append(text)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c86e9008-76ac-4ea8-9618-412cc2460fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_id_to_piece(token_id, sp_model):\n",
    "    \"\"\"Convert token ID to text piece, handling special tokens\"\"\"\n",
    "    if token_id == sp_model.pad_id():  # Typically -1\n",
    "        return \"<pad>\"\n",
    "    elif token_id == 0:\n",
    "        return \"<space>\"\n",
    "    elif token_id < 0 or token_id >= len(sp_model):\n",
    "        return f\"[INVALID_ID_{token_id}]\"\n",
    "    return sp_model.id_to_piece(int(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "24729ef7-75f3-4823-99ae-fc44bc268936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataset, num_classes, pad_id=None, normalize=True, smoothing_eps=0.0):\n",
    "    \"\"\"\n",
    "    Computes class weights for CrossEntropyLoss based on token frequency.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset object. Each sample must have 'texts' field (list of token IDs).\n",
    "        num_classes: Total number of classes (vocab size).\n",
    "        pad_id: Token ID to ignore (e.g., padding). If None, no ignoring.\n",
    "        normalize: If True, normalize weights to sum to 1.\n",
    "        smoothing_eps: Small value to add to all counts to prevent zero weights (optional).\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (num_classes,) with weights.\n",
    "    \"\"\"\n",
    "    token_counter = Counter()\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        token_ids = sample['text']\n",
    "        if isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "        token_counter.update(token_ids)\n",
    "\n",
    "    total_count = sum(token_counter.values())\n",
    "\n",
    "    class_weights = torch.zeros(num_classes)\n",
    "\n",
    "    for token_id in range(num_classes):\n",
    "        count = token_counter.get(token_id, 0)\n",
    "        if pad_id is not None and token_id == pad_id:\n",
    "            class_weights[token_id] = 0.0\n",
    "        else:\n",
    "            class_weights[token_id] = total_count / (count + smoothing_eps)\n",
    "\n",
    "    if normalize:\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92aa83-8b96-4d2a-b444-f9431f34c3e8",
   "metadata": {},
   "source": [
    "# Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d4999-eaeb-4edf-99ec-a52c17a13176",
   "metadata": {},
   "source": [
    "### Ctc Loss training\n",
    "Training Loop for PolishAVHubert Model\n",
    "1. Dataset Setup:\n",
    "\n",
    "SentencePiece Processor: The spm.SentencePieceProcessor is loaded with a pre-trained SentencePiece model to process text into subword units.\n",
    "\n",
    "Train and Validation Datasets:\n",
    "\n",
    "train_dataset and val_dataset are instances of DynamicAVHubertDataset, where the datasets are built by passing a list of video file paths and corresponding transcript paths (SRT files). These datasets are used to process the raw video data and text into the necessary format for training.\n",
    "\n",
    "For each video in the training and validation set, corresponding text transcriptions are used to provide target sequences for training.\n",
    "\n",
    "2. Data Loaders:\n",
    "\n",
    "train_dataloader and val_dataloader are instances of PyTorch’s DataLoader, which will load the train and validation datasets in batches of 8, respectively. These data loaders handle shuffling (for the training set) and ensure that videos and text are properly collated for input to the model.\n",
    "\n",
    "3. Model Initialization:\n",
    "\n",
    "Base Model Loading: The base model is loaded using checkpoint_utils.load_model_ensemble_and_task. The first model from the checkpoint is used as the initial model.\n",
    "\n",
    "Model Setup: The PolishAVHubert model is initialized by passing the base model and the SentencePiece model. It is moved to the GPU (if available) and wrapped with torch.compile for efficient execution.\n",
    "\n",
    "Optimizer and Scheduler: An AdamW optimizer is created for the model’s parameters that require gradients, and a learning rate scheduler (ReduceLROnPlateau) is initialized to adjust the learning rate when validation loss plateaus.\n",
    "\n",
    "4. Checkpointing:\n",
    "\n",
    "The training loop supports checkpointing. If a checkpoint exists and Config.resume_training is set to True, the model and optimizer states are loaded from the checkpoint, allowing for continuation from the last saved epoch.\n",
    "\n",
    "5. Training Loop:\n",
    "\n",
    "Epoch Loop: The training runs for a predefined number of epochs (Config.num_epochs).\n",
    "\n",
    "Training Phase: The model is set to train() mode, and the training loop processes the training batches:\n",
    "\n",
    "Video data is passed through the model, and text is encoded using the SentencePiece processor.\n",
    "\n",
    "The output of the model is passed through the CTC loss function (F.ctc_loss), which computes the error between the predicted and target sequences.\n",
    "\n",
    "Gradients are calculated, and the model is updated using the AdamW optimizer.\n",
    "\n",
    "The loss for each batch is accumulated for the epoch.\n",
    "\n",
    "Validation Phase: After completing the training phase, the model is evaluated in eval() mode:\n",
    "\n",
    "The validation loss is computed similarly to the training loss but without gradient updates.\n",
    "\n",
    "Word Error Rate (WER) is calculated to measure the model’s accuracy in transcribing the video content.\n",
    "\n",
    "The predictions and references (ground truth) are collected for calculating WER.\n",
    "\n",
    "6. Metrics Logging:\n",
    "\n",
    "Training and validation losses are logged to TensorBoard using writer.add_scalars, and the validation WER is logged with writer.add_scalar.\n",
    "\n",
    "7. Checkpoint Saving:\n",
    "\n",
    "The model is saved as a checkpoint whenever the validation loss improves. If no improvement is seen for a specified number of epochs (Config.early_stop_patience), the training process will stop early.\n",
    "\n",
    "The model, optimizer state, and other relevant information (epoch, losses, WER) are saved in the checkpoint file path.\n",
    "\n",
    "8. Learning Rate Scheduling:\n",
    "\n",
    "The learning rate is adjusted using scheduler.step(val_avg_loss) to reduce it when the validation loss stops improving.\n",
    "\n",
    "9. Logging:\n",
    "\n",
    "Throughout the loop, various training statistics, including train and validation loss, WER, and learning rate, are printed for monitoring progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7878428-e20e-4747-9cea-e8fd9ffb671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744990933.528392 2055653 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1744990933.576217 2370726 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n",
      "/home/jupyter-mmarton/cw_modelowanie2/audio-video-text-generation/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "/home/jupyter-mmarton/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Train Epoch 1: 100%|██████████| 3/3 [00:20<00:00,  6.73s/it]\n",
      "Val Epoch 1: 100%|██████████| 3/3 [00:19<00:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 107.2359 | Val Loss: 79.4203 | Val WER: 100.00% | LR: 3.00e-04\n",
      "net_output {'encoder_out': [tensor([[[ 0.1932, -0.0373,  0.0113,  ..., -0.1746, -0.0663,  0.0049]],\n",
      "\n",
      "        [[ 0.1932, -0.0372,  0.0112,  ..., -0.1745, -0.0661,  0.0047]],\n",
      "\n",
      "        [[ 0.1933, -0.0373,  0.0111,  ..., -0.1745, -0.0661,  0.0048]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1931, -0.0373,  0.0112,  ..., -0.1746, -0.0661,  0.0048]],\n",
      "\n",
      "        [[ 0.1931, -0.0373,  0.0112,  ..., -0.1746, -0.0662,  0.0049]],\n",
      "\n",
      "        [[ 0.1932, -0.0374,  0.0111,  ..., -0.1745, -0.0661,  0.0048]]],\n",
      "       device='cuda:0')], 'padding_mask': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]], device='cuda:0'), 'output_lengths': tensor([75], device='cuda:0')}\n",
      "all_references ['▁to ▁jest', '▁mnie', '▁te m u ▁co ▁chyba ▁większość ▁o s ó b ▁uważ a', '▁sprawa ▁otwartego ▁dostępu', '▁nie ▁jest ▁jak a ś', '▁re wo lu cyj na ▁in i cja ty wa', '▁tylko ▁ja', '▁po strze g ał ▁to ▁jako ▁na tu ra l ny', '▁s ytu a cji ▁w ▁świecie ▁je ś li ▁ch o dzi', '▁sp oso by ▁ko m u nika cji ▁mnie ▁się', '▁że ▁c i ▁któ rzy ▁dzisiaj ▁są ▁prze c i w ni', '▁dostęp o wi ▁b o ▁są ▁t a cy ▁nie', '▁szans ▁na ▁zwy cięstwo ▁krótko ▁mówiąc', '▁dominuje ▁w ▁tej ▁chwili ▁przekona nie', '▁potrzebie ▁i ▁sensie', '▁wy mieniania ▁in formacji ▁na', '▁temat ▁co ▁nie ▁zawsze ▁jest ▁korzystne']\n",
      "all_predictions ['są', 'są', 'są', 'są', 'swo', 'są', 'są', 'są', 'są', 'są', 'są', 'są', 'są', 'są', 'są', 'są', 'są']\n",
      "✅ New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 3/3 [00:20<00:00,  6.86s/it]\n",
      "Val Epoch 2: 100%|██████████| 3/3 [00:19<00:00,  6.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 98.7042 | Val Loss: 76.3558 | Val WER: 100.00% | LR: 3.00e-04\n",
      "net_output {'encoder_out': [tensor([[[ 0.7426, -0.1280,  0.0242,  ..., -0.2935, -0.1364, -0.1172]],\n",
      "\n",
      "        [[ 0.7425, -0.1278,  0.0242,  ..., -0.2935, -0.1361, -0.1173]],\n",
      "\n",
      "        [[ 0.7423, -0.1279,  0.0242,  ..., -0.2934, -0.1360, -0.1173]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.7423, -0.1279,  0.0242,  ..., -0.2934, -0.1363, -0.1175]],\n",
      "\n",
      "        [[ 0.7424, -0.1279,  0.0242,  ..., -0.2935, -0.1363, -0.1174]],\n",
      "\n",
      "        [[ 0.7424, -0.1281,  0.0242,  ..., -0.2933, -0.1364, -0.1173]]],\n",
      "       device='cuda:0')], 'padding_mask': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]], device='cuda:0'), 'output_lengths': tensor([75], device='cuda:0')}\n",
      "all_references ['▁to ▁jest', '▁mnie', '▁te m u ▁co ▁chyba ▁większość ▁o s ó b ▁uważ a', '▁sprawa ▁otwartego ▁dostępu', '▁nie ▁jest ▁jak a ś', '▁re wo lu cyj na ▁in i cja ty wa', '▁tylko ▁ja', '▁po strze g ał ▁to ▁jako ▁na tu ra l ny', '▁s ytu a cji ▁w ▁świecie ▁je ś li ▁ch o dzi', '▁sp oso by ▁ko m u nika cji ▁mnie ▁się', '▁że ▁c i ▁któ rzy ▁dzisiaj ▁są ▁prze c i w ni', '▁dostęp o wi ▁b o ▁są ▁t a cy ▁nie', '▁szans ▁na ▁zwy cięstwo ▁krótko ▁mówiąc', '▁dominuje ▁w ▁tej ▁chwili ▁przekona nie', '▁potrzebie ▁i ▁sensie', '▁wy mieniania ▁in formacji ▁na', '▁temat ▁co ▁nie ▁zawsze ▁jest ▁korzystne']\n",
      "all_predictions ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "✅ New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 3/3 [00:20<00:00,  6.71s/it]\n",
      "Val Epoch 3: 100%|██████████| 3/3 [00:19<00:00,  6.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 91.4306 | Val Loss: 63.8183 | Val WER: 100.00% | LR: 3.00e-04\n",
      "net_output {'encoder_out': [tensor([[[ 1.4228, -0.2118, -0.0331,  ..., -0.3806, -0.1552, -0.1023]],\n",
      "\n",
      "        [[ 1.4229, -0.2115, -0.0331,  ..., -0.3805, -0.1547, -0.1022]],\n",
      "\n",
      "        [[ 1.4228, -0.2116, -0.0331,  ..., -0.3803, -0.1547, -0.1022]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4227, -0.2117, -0.0332,  ..., -0.3804, -0.1548, -0.1024]],\n",
      "\n",
      "        [[ 1.4229, -0.2116, -0.0331,  ..., -0.3806, -0.1548, -0.1022]],\n",
      "\n",
      "        [[ 1.4228, -0.2118, -0.0332,  ..., -0.3803, -0.1549, -0.1022]]],\n",
      "       device='cuda:0')], 'padding_mask': tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]], device='cuda:0'), 'output_lengths': tensor([75], device='cuda:0')}\n",
      "all_references ['▁to ▁jest', '▁mnie', '▁te m u ▁co ▁chyba ▁większość ▁o s ó b ▁uważ a', '▁sprawa ▁otwartego ▁dostępu', '▁nie ▁jest ▁jak a ś', '▁re wo lu cyj na ▁in i cja ty wa', '▁tylko ▁ja', '▁po strze g ał ▁to ▁jako ▁na tu ra l ny', '▁s ytu a cji ▁w ▁świecie ▁je ś li ▁ch o dzi', '▁sp oso by ▁ko m u nika cji ▁mnie ▁się', '▁że ▁c i ▁któ rzy ▁dzisiaj ▁są ▁prze c i w ni', '▁dostęp o wi ▁b o ▁są ▁t a cy ▁nie', '▁szans ▁na ▁zwy cięstwo ▁krótko ▁mówiąc', '▁dominuje ▁w ▁tej ▁chwili ▁przekona nie', '▁potrzebie ▁i ▁sensie', '▁wy mieniania ▁in formacji ▁na', '▁temat ▁co ▁nie ▁zawsze ▁jest ▁korzystne']\n",
      "all_predictions ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "✅ New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 3/3 [00:19<00:00,  6.64s/it]\n",
      "Val Epoch 4:   0%|          | 0/3 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(val_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    161\u001b[0m         videos \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    162\u001b[0m         texts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mDynamicAVHubertDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m end_time \u001b[38;5;241m=\u001b[39m start_time \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Extract frames for the chunk\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:  \u001b[38;5;66;03m# Minimum 10 frames\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic:\n",
      "Cell \u001b[0;32mIn[6], line 139\u001b[0m, in \u001b[0;36mDynamicAVHubertDataset._extract_frames\u001b[0;34m(self, video_path, start_time, end_time, width, height)\u001b[0m\n\u001b[1;32m    136\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"sp_model.model\")\n",
    "train_dataset = DynamicAVHubertDataset(\n",
    "    video_paths=[video_files_path / \"duda_wywiad.mp4\",\n",
    "                 video_files_path / \"geologia_2.mp4\",\n",
    "                 video_files_path / \"geologia_3.mp4\",\n",
    "                 video_files_path / \"reklama_kobieta.mp4\",\n",
    "                 video_files_path / \"bosak_wywiad.mp4\",\n",
    "                 video_files_path / \"sport.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"duda.srt\",\n",
    "                      transcriptions_path / \"geologia 2.srt\",\n",
    "                      transcriptions_path / \"geologia_3.srt\",\n",
    "                      transcriptions_path / \"reklama kobieta.srt\",\n",
    "                      transcriptions_path / \"bosak wywiad.srt\",\n",
    "                      transcriptions_path / \"sport.srt\",\n",
    "                    ],\n",
    "    sp_model=sp\n",
    ")\n",
    "\n",
    "val_dataset = DynamicAVHubertDataset(\n",
    "    video_paths=[video_files_path / \"geologia_4.mp4\",\n",
    "                 video_files_path / \"profesor.mp4\",\n",
    "                 video_files_path / \"sport2.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"geologia_4.srt\",\n",
    "                      transcriptions_path / \"profesor.srt\",\n",
    "                      transcriptions_path / \"sport2.srt\",\n",
    "                    ],\n",
    "    sp_model=sp\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 2. Initialize Model and Training Components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([CHECKPOINT_PATH])\n",
    "model_initial = models[0]\n",
    "\n",
    "torch.backends.cudnn.benchmark = True \n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "model = PolishAVHubert(model_initial, \"sp_model.model\").to(device)\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=Config.learning_rate\n",
    ")\n",
    "ctc_cfg = CustomCtcConfig()\n",
    "criterion = CtcCriterion(ctc_cfg, task).to(device)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "best_wer = float('inf')\n",
    "best_val_avg_loss = float('inf')\n",
    "start_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "if Path(Config.checkpoint_path).exists() and Config.resume_training is True:\n",
    "    checkpoint = torch.load(Config.checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_wer = checkpoint['val_wer']\n",
    "    print(f\"✅ Loaded checkpoint from epoch {start_epoch}, best WER so far: {best_wer:.2%}\")\n",
    "\n",
    "for epoch in range(start_epoch, Config.num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        videos = batch['video'].to(device)\n",
    "        texts = batch['text']\n",
    "        video_lengths = batch['lengths'].to(device)\n",
    "        #videos = videos.mean(dim=2, keepdim=True)\n",
    "        \n",
    "        targets, target_lengths = encode_text(texts, sp)\n",
    "\n",
    "        targets = targets + 1\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = (targets != 0).sum(dim=1)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        targets = targets[targets != 0]\n",
    "        \n",
    "        sample = {\n",
    "            \"net_input\": {\"video\": videos.permute(0, 2, 1, 3, 4)},\n",
    "            \"target\": targets,\n",
    "            \"target_lengths\": target_lengths,\n",
    "            \"ntokens\": target_lengths.sum().item(),\n",
    "            \"id\": torch.arange(videos.size(0))\n",
    "        }\n",
    "        net_output = model(sample[\"net_input\"][\"video\"])\n",
    "        log_probs = model.get_normalized_probs(net_output, log_probs=True)\n",
    "        input_lengths = video_lengths\n",
    "        \n",
    "        loss = F.ctc_loss(\n",
    "            log_probs,\n",
    "            targets,\n",
    "            input_lengths,\n",
    "            target_lengths,\n",
    "            blank=0,\n",
    "            reduction='mean',\n",
    "            zero_infinity=True\n",
    "        ).to(device)\n",
    "        loss_value = loss[0] if isinstance(loss, tuple) else loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += loss_value.item()\n",
    "        \n",
    "    # Validation Phase (keep full precision)\n",
    "    model.eval()\n",
    "    val_epoch_loss = 0\n",
    "    all_references = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Val Epoch {epoch+1}\"):\n",
    "            videos = batch['video'].to(device)\n",
    "            texts = batch['text']\n",
    "            video_lengths = batch['lengths'].to(device)\n",
    "\n",
    "            videos = videos.mean(dim=2, keepdim=True)\n",
    "            \n",
    "            targets, target_lengths = encode_text(texts, sp)\n",
    "    \n",
    "            targets = targets + 1\n",
    "            targets = targets.to(device)\n",
    "            target_lengths = (targets != 0).sum(dim=1)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            targets = targets[targets != 0]\n",
    "                \n",
    "            sample = {\n",
    "                \"net_input\": {\"video\": videos.permute(0, 2, 1, 3, 4)},\n",
    "                \"target\": targets,\n",
    "                \"target_lengths\": target_lengths,\n",
    "                \"ntokens\": target_lengths.sum().item(),\n",
    "                \"id\": torch.arange(videos.size(0))\n",
    "            }\n",
    "            \n",
    "            net_output = model(sample[\"net_input\"][\"video\"])  # still on GPU\n",
    "            log_probs = model.get_normalized_probs(net_output, log_probs=True)\n",
    "            input_lengths = video_lengths // 2\n",
    "            \n",
    "            loss_value = F.ctc_loss(\n",
    "                log_probs,\n",
    "                targets,\n",
    "                input_lengths,\n",
    "                target_lengths,\n",
    "                blank=0,\n",
    "                reduction='mean',\n",
    "                zero_infinity=True\n",
    "            ).to(device)\n",
    "            loss_value = loss[0] if isinstance(loss, tuple) else loss\n",
    "            val_epoch_loss += loss_value.item()\n",
    "            \n",
    "            net_output = model(videos.permute(0, 2, 1, 3, 4))\n",
    "            log_probs = model.get_normalized_probs(net_output, log_probs=True)\n",
    "            input_lengths = video_lengths\n",
    "            predictions = decode_predictions(log_probs, model.sp, input_lengths, blank_id=0)\n",
    "\n",
    "            all_references.extend(texts)\n",
    "            all_predictions.extend(predictions)\n",
    "    \n",
    "    train_avg_loss = train_epoch_loss / len(train_dataloader)\n",
    "    val_avg_loss = val_epoch_loss / len(val_dataloader)\n",
    "    val_wer = wer(all_references, all_predictions)\n",
    "    \n",
    "    writer.add_scalars('Loss', {\n",
    "        'train': train_avg_loss,\n",
    "        'val': val_avg_loss\n",
    "    }, epoch)\n",
    "    writer.add_scalar('WER/val', val_wer, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | \"\n",
    "          f\"Train Loss: {train_avg_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_avg_loss:.4f} | \"\n",
    "          f\"Val WER: {val_wer:.2%} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    print(\"net_output\", net_output)\n",
    "    print(\"all_references\", all_references)\n",
    "    print(\"all_predictions\", all_predictions)\n",
    "    \n",
    "    #if val_wer < best_wer:\n",
    "    if val_avg_loss < best_val_avg_loss:\n",
    "        #best_wer = val_wer\n",
    "        best_val_avg_loss = val_avg_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_avg_loss,\n",
    "            'val_loss': val_avg_loss,\n",
    "            'val_wer': val_wer,\n",
    "        }, Config.checkpoint_path)\n",
    "        print(\"✅ New best model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= Config.early_stop_patience:\n",
    "            break\n",
    "    \n",
    "    #scheduler.step(val_wer)\n",
    "    scheduler.step(val_avg_loss)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fe025-6e7a-4d2e-ad4d-066a1e2a65d4",
   "metadata": {},
   "source": [
    "### CrossEntropy training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e26e49-4488-4b2d-9587-0b2ba4bee18e",
   "metadata": {},
   "source": [
    "Sanity check was preformed on one chunk of data to see if model is able to overfit to data. If not that means that there is error in setup and model wont be able to learn to generalize to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fec3d760-0ee6-481b-8a96-183089b4e8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745263178.341218 2567920 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745263178.398162  701632 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n",
      "/home/jupyter-mmarton/cw_modelowanie2/audio-video-text-generation/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "/home/jupyter-mmarton/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3000, Loss: 6.8811\n",
      "🗣️  Reference : ['▁to', '▁to', '▁to', '▁to', '▁to']\n",
      "🤖 Prediction: ['▁springe', '▁my', '▁wiadomość', '▁instytuty', 'to']\n",
      "--------------------------------------------------\n",
      "Epoch 100/3000, Loss: 1.0532\n",
      "🗣️  Reference : ['▁to', '▁to', '▁to', '▁to', '▁to']\n",
      "🤖 Prediction: ['▁to', '▁to', '▁to', '▁to', '▁to']\n",
      "--------------------------------------------------\n",
      "Epoch 200/3000, Loss: 0.4965\n",
      "🗣️  Reference : ['▁to', '▁to', '▁to', '▁to', '▁to']\n",
      "🤖 Prediction: ['▁to', '▁to', '▁to', '▁to', '▁to']\n",
      "--------------------------------------------------\n",
      "✅ Model has successfully learned the batch!\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"sp_model.model\")\n",
    "\n",
    "# Dataset - now using frame-level version\n",
    "train_dataset = DynamicAVHubertDatasetFrames(\n",
    "    video_paths=[video_files_path / \"train_1min.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"train 1min.srt\"],\n",
    "    sp_model=sp,\n",
    "    deterministic=True,\n",
    "    fps=25  # frames per second\n",
    ")\n",
    "\n",
    "# Load pretrained AV-HuBERT model\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([CHECKPOINT_PATH])\n",
    "model_initial = models[0]\n",
    "model = PolishAVHubertSingleOutput(model_initial, \"sp_model.model\").to(device)\n",
    "\n",
    "# Compile model (optional, for speedup)\n",
    "model = torch.compile(model)\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Create training chunks\n",
    "train_batches = create_frame_batches(train_dataset, batch_size=32)\n",
    "\n",
    "# Sample and optimizer\n",
    "sample_batch = train_batches[1]  # pick first chunk\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Prepare batch\n",
    "    videos = torch.stack([sample['video'] for sample in sample_batch]).to(device)  # (B, 1, H, W)\n",
    "    videos = videos.unsqueeze(1)  # (B, 1, 1, H, W) - adding channel dim\n",
    "    \n",
    "    # Prepare targets\n",
    "    texts = [sample['text'] for sample in sample_batch]\n",
    "    targets = torch.tensor(texts, dtype=torch.long).to(device)  # (B,)\n",
    "\n",
    "    # Forward pass - now gets single prediction per frame\n",
    "    net_output = model(videos)\n",
    "    logits = net_output[\"encoder_out\"].squeeze(1)  # (B, vocab_size)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits, targets)\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Decode predictions\n",
    "    predicted_ids = torch.argmax(logits, dim=1)  # (B,)\n",
    "    pred_texts = [sp.id_to_piece(int(id)) for id in predicted_ids]\n",
    "\n",
    "    # Logging\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/3000, Loss: {loss.item():.4f}\")\n",
    "        print(f\"🗣️  Reference : {[sp.id_to_piece(t) for t in texts[:5]]}\")\n",
    "        print(f\"🤖 Prediction: {pred_texts[:5]}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Early stopping if all predictions match\n",
    "    if all(p == t for p, t in zip(pred_texts, [sp.id_to_piece(t) for t in texts])):\n",
    "        print(\"✅ Model has successfully learned the batch!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc864ffe-199a-41bd-a7cf-c7ca57bb6852",
   "metadata": {},
   "source": [
    "Below training loop for CrossEntropy works in simillar way as previous Ctc training loop with few key differences.\n",
    "\n",
    "Frame-Level CrossEntropy predicts one token per frame directly and each video frame is expected to match exactly one subword (no flexibility).\n",
    "\n",
    "CTC Loss predicts a sequence of tokens from a sequence of frames. CTC handles variable-length outputs: it allows multiple frames to map to a single token or insert blank tokens automatically\n",
    "\n",
    "For Accuracy evaluation, frame level crossentropy accuracy is computed frame by frame and for CTC Loss Accuracy is based on sequence matching (e.g., Word Error Rate - WER), comparing predicted text to ground truth text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "6d2aa8db-2a85-46bd-b6e6-969205658272",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745709714.022000  860211 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745709714.079409  268562 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n",
      "I0000 00:00:1745709715.737103  860211 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745709715.789810  269196 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n",
      "/home/jupyter-mmarton/cw_modelowanie2/audio-video-text-generation/av_hubert/fairseq/fairseq/checkpoint_utils.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "/home/jupyter-mmarton/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Train Epoch 1:  35%|███▍      | 660/1898 [1:31:48<2:52:12,  8.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[409], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    109\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[281], line 73\u001b[0m, in \u001b[0;36mDynamicAVHubertDatasetFrames.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     70\u001b[0m original_frame_time \u001b[38;5;241m=\u001b[39m frame_idx \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_fps\n\u001b[1;32m     71\u001b[0m frame_time \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_duration  \u001b[38;5;66;03m# More precise timing\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_single_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentry_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_frame_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentry_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentry_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_size)),\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp\u001b[38;5;241m.\u001b[39mpad_id()  \u001b[38;5;66;03m# or appropriate blank token\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[281], line 130\u001b[0m, in \u001b[0;36mDynamicAVHubertDatasetFrames._extract_single_frame\u001b[0;34m(self, video_path, timestamp, width, height)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE)\n\u001b[0;32m--> 130\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mterminate()\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"sp_model.model\")\n",
    "\n",
    "train_dataset = DynamicAVHubertDatasetFrames(\n",
    "    video_paths=[video_files_path / \"geologia_1.mp4\",\n",
    "                 video_files_path / \"geologia_2.mp4\",\n",
    "                 video_files_path / \"geologia_3.mp4\",\n",
    "                 video_files_path / \"geologia_4.mp4\",\n",
    "                 video_files_path / \"geologia_5.mp4\",\n",
    "                 video_files_path / \"geologia_6.mp4\",\n",
    "                 video_files_path / \"geologia_7.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"geologia 1.srt\",\n",
    "                      transcriptions_path / \"geologia 2.srt\",\n",
    "                      transcriptions_path / \"geologia_3.srt\",\n",
    "                      transcriptions_path / \"geologia_4.srt\",\n",
    "                      transcriptions_path / \"geologia 5.srt\",\n",
    "                      transcriptions_path / \"geologia 6.srt\",\n",
    "                      transcriptions_path / \"geologia 7.srt\",\n",
    "                    ],\n",
    "    sp_model=sp,\n",
    "    fps=25,\n",
    "    frame_interval=5\n",
    ")\n",
    "\n",
    "val_dataset = DynamicAVHubertDatasetFrames(\n",
    "    video_paths=[video_files_path / \"geologia_8.mp4\",\n",
    "                 video_files_path / \"geologia_9.mp4\",\n",
    "                 video_files_path / \"geologia_10.mp4\"],\n",
    "    transcript_paths=[transcriptions_path / \"geologia 8.srt\",\n",
    "                      transcriptions_path / \"geologia 9.srt\",\n",
    "                      transcriptions_path / \"geologia 10.srt\",\n",
    "                    ],\n",
    "    sp_model=sp,\n",
    "    fps=25,\n",
    "    frame_interval=5\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_frames,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_frames,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Initialize model and training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([CHECKPOINT_PATH])\n",
    "model_initial = models[0]\n",
    "\n",
    "num_classes = sp.get_piece_size()   # if you use SentencePiece\n",
    "pad_token_id = sp.pad_id()           # or whatever your pad id is\n",
    "\n",
    "class_weights = compute_class_weights(\n",
    "    dataset=train_dataset,\n",
    "    num_classes=num_classes,\n",
    "    pad_id=pad_token_id,\n",
    "    normalize=True,\n",
    "    smoothing_eps=1e-6\n",
    ")\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Performance optimizations\n",
    "torch.backends.cudnn.benchmark = True \n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model = PolishAVHubertSingleOutput(model_initial, \"sp_model.model\").to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=sp.pad_id())#model.sp.pad_id())#model.sp.pad_id())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "# Training state\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Load checkpoint if resuming\n",
    "if Path(Config.checkpoint_path).exists() and Config.resume_training:\n",
    "    checkpoint = torch.load(Config.checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_loss = checkpoint['val_loss']\n",
    "    print(f\"✅ Loaded checkpoint from epoch {start_epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, Config.num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\")):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        # Prepare batch\n",
    "        videos = batch['videos'].unsqueeze(1).to(device)  # (B, 1, 1, H, W)\n",
    "        targets = torch.tensor(batch['texts'], dtype=torch.long).to(device)  # (B,)\n",
    "\n",
    "        # Forward pass\n",
    "        net_output = model(videos)\n",
    "        logits = net_output[\"encoder_out\"].squeeze(1)  # (B, vocab_size)\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, targets) \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.max_grad_norm)\n",
    " \n",
    "        optimizer.step()\n",
    " \n",
    "        # Statistics\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "\n",
    "        train_total += targets.size(0)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "  \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            videos = batch['videos'].unsqueeze(1).to(device)\n",
    "            targets = torch.tensor(batch['texts'], dtype=torch.long).to(device)\n",
    "            \n",
    "            net_output = model(videos)\n",
    "            logits = net_output[\"encoder_out\"].squeeze(1)\n",
    "            \n",
    "            loss = criterion(logits, targets)\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            # Store predictions for analysis\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "    # Calculate metrics\n",
    "    train_avg_loss = train_loss / train_total\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    val_avg_loss = val_loss / val_total\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    # Sample predictions\n",
    "    sample_preds = [safe_id_to_piece(p, sp) for p in all_preds[:10]]\n",
    "    sample_targets = [safe_id_to_piece(t, sp) for t in all_targets[:10]]\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalars('Loss', {'train': train_avg_loss, 'val': val_avg_loss}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'train': train_acc, 'val': val_acc}, epoch)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{Config.num_epochs}\")\n",
    "    print(f\"Train Loss: {train_avg_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_avg_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Samples:\\nPred: {sample_preds}\\nTrue: {sample_targets}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Checkpointing\n",
    "    if val_avg_loss < best_val_loss:\n",
    "        best_val_loss = val_avg_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_avg_loss,\n",
    "            'val_loss': val_avg_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }, Config.checkpoint_path)\n",
    "        print(\"✅ New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= Config.early_stop_patience:\n",
    "            print(f\"Early stopping after {Config.early_stop_patience} epochs without improvement\")\n",
    "            break\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_avg_loss)\n",
    "\n",
    "writer.close()\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jupyter Safe)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
